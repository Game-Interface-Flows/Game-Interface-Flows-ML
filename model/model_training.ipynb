{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from dataset import DataModule\n",
    "from model import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(config.RANDOM_SEED)\n",
    "pl.seed_everything(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evlko/Documents/GitHub/Game-Interface-Flows-ML/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/evlko/Documents/GitHub/Game-Interface-Flows-ML/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "dm = DataModule(\n",
    "    data_path=config.DATA_PATH,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    train_test_ratio=config.TRAIN_TEST_RATIO,\n",
    "    train_val_ratio=config.TRAIN_VAL_RATIO,\n",
    ")\n",
    "\n",
    "model = ClassificationModel(learning_rate=config.LEARNING_RATE)\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=config.ACCELERATOR,\n",
    "    devices=config.DEVICES,\n",
    "    min_epochs=config.MIN_EPOCHS,\n",
    "    max_epochs=config.MAX_EPOCHS,\n",
    "    enable_checkpointing=False,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | ResNet           | 11.2 M\n",
      "1 | loss_fn   | CrossEntropyLoss | 0     \n",
      "2 | accuracy  | BinaryAccuracy   | 0     \n",
      "3 | precision | BinaryPrecision  | 0     \n",
      "-----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 50/50 [00:28<00:00,  1.72it/s, v_num=9, val_acc=0.909, val_precision=0.850]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 50/50 [00:29<00:00,  1.72it/s, v_num=9, val_acc=0.909, val_precision=0.850]\n",
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:01<00:00,  5.51it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc             0.903124988079071\n",
      "        val_loss            0.2872771620750427\n",
      "      val_precision         0.8376444578170776\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Testing DataLoader 0: 100%|██████████| 3/3 [00:00<00:00,  6.87it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.8999999761581421\n",
      "        test_loss           0.28641724586486816\n",
      "     test_precision         0.8333333134651184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.28641724586486816,\n",
       "  'test_acc': 0.8999999761581421,\n",
       "  'test_precision': 0.8333333134651184}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, dm)\n",
    "trainer.validate(model, dm)\n",
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test images: 90.75 %\n",
      "1452\n"
     ]
    }
   ],
   "source": [
    "test_loader = dm.train_dataloader()\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on test images: {100 * correct / total} %\")\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сумма вышла (71 + 289 + ...) / 2000 = 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in the test DataLoader: 1600\n"
     ]
    }
   ],
   "source": [
    "def count_images_in_dataloader(dataloader):\n",
    "    total_images = 0\n",
    "    for _, labels in dataloader:\n",
    "        total_images += labels.size(\n",
    "            0\n",
    "        )  # Assuming labels are batched similarly to images\n",
    "    return total_images\n",
    "\n",
    "\n",
    "# Example usage with your test DataLoader\n",
    "test_loader = dm.train_dataloader()\n",
    "total_images = count_images_in_dataloader(test_loader)\n",
    "print(f\"Total number of images in the test DataLoader: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80 + 320 + 1600  # something is wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on all images: 90.30 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "import pytorch_lightning as pl\n",
    "from albumentation_transforms import AlbumentationTransforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "\n",
    "# Step 1: Setup the transformations, ensure these are the same as used in your training minus any augmentations\n",
    "transform = AlbumentationTransforms(\n",
    "    A.Compose(\n",
    "        [\n",
    "            A.Resize(width=256, height=256),\n",
    "            A.ToGray(always_apply=True),\n",
    "            A.Normalize(mean=[0.485], std=[0.229]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 2: Load your entire dataset\n",
    "all_data = datasets.ImageFolder(root=\"data/\", transform=transform)\n",
    "\n",
    "# Step 3: Create a DataLoader\n",
    "all_data_loader = DataLoader(all_data, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Step 4: Calculate the accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in all_data_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the network on all images: {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(width=256, height=256),\n",
    "            A.ToGray(always_apply=True),\n",
    "            A.Normalize(mean=[0.485], std=[0.229]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def load_and_transform_image(image_path, transform):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    image_np = np.array(image)\n",
    "    transformed = transform(image=image_np)[\"image\"]\n",
    "    transformed = transformed.unsqueeze(0)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "# Define the path to your image\n",
    "image_path = \"data/1/anthem_1.jpg\"\n",
    "\n",
    "# Get the transformation function ready\n",
    "transform = get_transform()\n",
    "\n",
    "# Load and transform the image\n",
    "transformed_image = load_and_transform_image(image_path, transform)\n",
    "\n",
    "# Predict with the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(transformed_image)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# Print or return the result\n",
    "print(f\"Predicted class: {predicted.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"resnet18_weights.pth\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def create_model(num_classes=2):\n",
    "    # Load a pretrained ResNet18 model\n",
    "    model = models.resnet18(weights=None)\n",
    "    # Modify the fully connected layer to match the number of classes\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "saved_model = create_model(num_classes=2)\n",
    "\n",
    "state_dict = torch.load(\"resnet18_weights.pth\")\n",
    "\n",
    "adjusted_state_dict = {\n",
    "    key.replace(\"model.\", \"\"): value for key, value in state_dict.items()\n",
    "}\n",
    "\n",
    "saved_model.load_state_dict(adjusted_state_dict)\n",
    "\n",
    "saved_model.eval()\n",
    "\n",
    "\n",
    "def predict_image(image_path, model, transform):\n",
    "    image = load_and_transform_image(image_path, transform)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "image_path = \"data/1/anthem_1.jpg\"\n",
    "predicted_class = predict_image(image_path, model, get_transform())\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
